# üß© 0. Pr√©parer l‚Äôenvironnement kolla-ansible (venv + d√©pendances)

üëâ √Ä faire **en root** sur l‚Äôh√¥te (pas dans un container).

```bash
sudo -i
# Passe en root (shell de connexion root). Obligatoire car :
# - on va installer des paquets syst√®me
# - on va cr√©er un venv dans /root
# - kolla-ansible s'utilise g√©n√©ralement depuis root

apt update
# Met √† jour la liste des paquets disponibles (index APT).
# Toujours √† faire avant un apt install sur une machine qui n‚Äôest pas √† jour.

apt install -y python3-venv python3-dev libffi-dev gcc libssl-dev libyaml-dev
# Installe les d√©pendances de base pour faire tourner kolla-ansible dans un virtualenv :
# - python3-venv : permet de cr√©er des environnements virtuels Python (python -m venv)
# - python3-dev  : headers Python pour compiler certains modules natifs
# - libffi-dev   : utilis√© par des libs comme cryptography (gestion des FFI, Foreign Function Interface)
# - gcc          : compilateur C n√©cessaire pour compiler des modules Python natifs
# - libssl-dev   : headers OpenSSL, utilis√©s par cryptography, TLS, etc.
# - libyaml-dev  : pour PyYAML (lecture des fichiers YAML d'Ansible et de Kolla)
```

Cr√©er un virtualenv d√©di√© pour kolla-ansible :

```bash
python3 -m venv /root/kolla-openstack
# Cr√©e un environnement virtuel Python isol√© dans /root/kolla-openstack.
# Avantages :
# - tu figes les versions de ansible / kolla-ansible / cryptography
# - tu √©vites les conflits avec les paquets syst√®me Python

source /root/kolla-openstack/bin/activate
# Active le virtualenv : √† partir de l√†, "python" et "pip" pointent vers l'env virtuel.
# Tout ce que tu installes via pip reste confin√© dans /root/kolla-openstack.
```

Mettre √† jour `pip`/`setuptools` pour √©viter l‚Äôerreur `setuptools_rust` avec `cryptography` :

```bash
python -m pip install --upgrade "pip==21.3.1" "setuptools<60" "wheel"
# On force :
# - une version de pip raisonnablement r√©cente (21.3.1)
# - setuptools < 60 pour √©viter les d√©pendances √† Rust sur d'anciennes versions de Python
# - wheel : permet d‚Äôinstaller des paquets Python pr√©compil√©s (format .whl)
# Pourquoi ? Pour que cryptography < 3.5 puisse s‚Äôinstaller SANS Rust sur Ubuntu/Python ancien.
```

Installer une version de `cryptography` compatible Python 3.6 **sans Rust** :

```bash
pip install "cryptography==3.4.8"
# On fixe cryptography √† 3.4.8, derni√®re version √† bien fonctionner sans Rust sur Python ancien.
# Cela √©vite :
# - les erreurs du type "setuptools-rust required for building..."
# - les probl√®mes de compilation sur une VM un peu "l√©g√®re".
```

Installer kolla-ansible + ansible dans ce venv :

```bash
pip install "ansible==2.9.27" "kolla-ansible==10.2.0"
# On installe des versions coh√©rentes avec Ussuri :
# - ansible 2.9.x : derni√®re branche support√©e officiellement par kolla-ansible Ussuri/Train/Stein
# - kolla-ansible 10.2.0 : branche qui correspond √† OpenStack Ussuri
# Tout cela reste DANS le venv : pas de pollution du syst√®me.
```

V√©rifier :

```bash
kolla-ansible --version
# V√©rifie que la commande kolla-ansible fonctionne et est bien celle du venv.

ansible --version
# V√©rifie la version de Ansible utilis√©e par le venv (2.9.27 normalement).
```

> üîÅ **√Ä chaque fois que tu veux utiliser kolla-ansible** :
>
> ```bash
> sudo -i
> source /root/kolla-openstack/bin/activate
> ```
>
> Sinon tu risques d‚Äôappeler un ansible/kolla-ansible du syst√®me (mauvaise version).

---

# üóÇ 1. V√©rifier / pr√©parer l‚Äôinventaire `all-in-one`

On va utiliser ton inventaire **AIO** (fichier `all-in-one`) et v√©rifier que Swift est bien mapp√© sur `localhost`.

```ini
[control]
localhost       ansible_connection=local
# Groupe "control" : n≈ìud qui h√©berge les services de contr√¥le
# (Keystone, Glance, Nova-API, Neutron-server, Horizon, Swift proxy dans ton cas).
# Ici : tout tourne sur localhost ‚Üí AIO (All-In-One).

[network]
localhost       ansible_connection=local
# Groupe "network" : n≈ìud qui h√©berge les agents r√©seau (Neutron L3, DHCP, etc.).
# Toujours localhost en AIO.

[compute]
localhost       ansible_connection=local
# Groupe "compute" : n≈ìud(s) qui h√©bergent les hyperviseurs Nova.
# Ici : Nova-compute tourne aussi sur localhost.

[storage]
localhost       ansible_connection=local
# Groupe "storage" : n≈ìud(s) de stockage (Cinder, Swift, etc.).
# En AIO, on utilise aussi localhost.

[monitoring]
localhost       ansible_connection=local
# Groupe pour Prometheus, Grafana, etc. sur localhost.

[deployment]
localhost       ansible_connection=local
# N≈ìud qui ex√©cute kolla-ansible. Dans un AIO, c'est aussi localhost.

# ...

[swift:children]
control
# Groupe "swift" = alias logique qui regroupe les n≈ìuds concern√©s par Swift (proxy).
# Ici, il h√©rite des n≈ìuds du groupe [control], donc localhost.

[swift-proxy-server:children]
swift
# Groupe pour les containers swift_proxy_server.
# Il utilise les h√¥tes du groupe [swift], donc les "control nodes".

[swift-account-server:children]
storage
# Containers swift_account_server d√©ploy√©s sur le groupe [storage]
# (dans un vraies archi prod : tu peux s√©parer control et storage).

[swift-container-server:children]
storage
# Containers swift_container_server √©galement sur les n≈ìuds de stockage.

[swift-object-server:children]
storage
# Containers swift_object_server (= data) sur les n≈ìuds de stockage.
```

‚úÖ Pour un AIO, ce mapping signifie :
**tout Swift (proxy + data) tourne sur la m√™me VM**, mais **logiquement s√©par√©** en r√¥les.

Juste pour le ranger proprement :

```bash
mkdir -p /etc/kolla
# Cr√©e le r√©pertoire standard de configuration de Kolla-Ansible.

cp /home/devops/all-in-one /etc/kolla/all-in-one
# Copie ton inventaire AIO dans /etc/kolla.
# Tu pourras lancer :
#   kolla-ansible -i /etc/kolla/all-in-one ...
# depuis n'importe o√π.
```

---

# üíΩ 2. Ajouter et pr√©parer le disque Swift

## 2.1. Ajouter un disque dans VirtualBox

L√†, c‚Äôest purement ‚Äúinfrastructure‚Äù : on ajoute un disque d√©di√© au stockage Swift.

1. √âteindre la VM.
2. VirtualBox ‚Üí **Param√®tres ‚Üí Stockage**.
3. Ajouter un nouveau disque VDI (20 Go, dynamique).
4. Red√©marrer la VM, puis :

```bash
lsblk
# Liste tous les disques et partitions.
# Tu dois voir un nouveau disque sans partitions, par ex. "sdb" de ~20G.
```

---

## 2.2. Partitionner + formater en XFS avec label `SWIFT_DATA`

```bash
sudo parted /dev/sdb --script mklabel gpt
# Initialise le disque /dev/sdb avec un label GPT (table de partitions moderne).
# ATTENTION : cela efface tout contenu pr√©c√©dent sur /dev/sdb.

sudo parted /dev/sdb --script mkpart primary 0% 100%
# Cr√©e une partition unique "primary" qui occupe 100% du disque.
# R√©sultat attendu : /dev/sdb1.
```

Formater avec un **label court (‚â§ 12 caract√®res)** :

```bash
sudo mkfs.xfs -f -L SWIFT_DATA /dev/sdb1
# Formate la partition /dev/sdb1 avec le syst√®me de fichiers XFS.
# - -f : force le formatage (m√™me si un FS existait).
# - -L SWIFT_DATA : assigne un LABEL au FS (max 12 chars).
# Swift, avec "swift_devices_match_mode: strict", se base sur ce LABEL.
```

V√©rifier :

```bash
lsblk -f
# -f affiche les infos de FS : type (xfs), LABEL, UUID.
# Tu dois voir sdb1 avec :
#   FSTYPE=xfs  LABEL=SWIFT_DATA
```

---

## 2.3. Monter le disque sur `/srv/node/sdb1`

```bash
sudo mkdir -p /srv/node/sdb1
# R√©pertoire de montage utilis√© par Swift.
# Convention : /srv/node/<nom_device_logique>
```

```bash
echo 'LABEL=SWIFT_DATA /srv/node/sdb1 xfs defaults 0 0' | sudo tee -a /etc/fstab
# Ajoute une entr√©e dans /etc/fstab pour que le FS soit mont√© automatiquement au boot :
# - LABEL=SWIFT_DATA : on cible le FS par son label, pas par /dev/sdb1 (plus robuste)
# - /srv/node/sdb1   : point de montage
# - xfs              : type de FS
# - defaults         : options de montage standard
# - 0 0              : pas de dump, pas de fsck au boot
```

```bash
sudo mount -a
# Relit /etc/fstab et monte tous les syst√®mes de fichiers non encore mont√©s.
```

```bash
df -h | grep /srv || echo "AUCUN_MONTAGE_SRV"
# V√©rifie que /srv/node/sdb1 est bien mont√© :
# - df -h : liste les FS mont√©s et leur utilisation
# - grep /srv : filtre ceux mont√©s sous /srv
# Si rien ne ressort, le echo te dit qu'aucun montage /srv n'existe.
```

---

## 2.4. Permissions UID pour l‚Äôutilisateur `swift` (point cl√©)

L√† on s‚Äôassure que **le FS sur l‚Äôh√¥te** appartient au m√™me `uid:gid` que l‚Äôutilisateur `swift` **dans les containers**.
Sinon Swift va r√¢ler ‚Äúpermission denied‚Äù en √©criture.

```bash
docker exec -it swift_account_server id swift
# On ex√©cute "id swift" DANS un container Swift (swift_account_server ici).
# Pourquoi ?
# Parce que dans Kolla, les UID/GID sont fix√©s (42445, 42446, etc.)
# et peuvent ne pas correspondre √† un utilisateur local de l‚Äôh√¥te.
#
# Ce qui nous int√©resse, c‚Äôest l'UID NUM√âRIQUE de swift dans le container.
```

Tu verras quelque chose comme :

```text
uid=42445(swift) gid=42445(swift) groups=42445(swift)
```

C‚Äôest cet UID que tu dois utiliser sur l‚Äôh√¥te :

```bash
sudo chown -R 42445:42445 /srv/node/sdb1
# Change le propri√©taire du FS mont√© pour qu'il appartienne √† l'utilisateur "swift"
# tel qu'il est vu DANS le container (UID 42445).
# Sans √ßa, les containers Swift ne pourront pas √©crire leurs donn√©es sur le disque.

sudo chmod -R 755 /srv/node/sdb1
# Donne les droits rx √† tout le monde et w au propri√©taire.
# Suffisant pour un lab (en prod on ferait plus fin, mais c'est OK).
```

V√©rifier :

```bash
ls -lan /srv/node
ls -lan /srv/node/sdb1
# -l   : long listing
# -n   : affiche les UID/GID num√©riques au lieu des noms.
# Tu dois voir :
#   drwxr-xr-x  42445  42445  ... sdb1
# ce qui confirme que swift (UID 42445) est propri√©taire.
```

---

# ‚öôÔ∏è 3. Configuration Swift dans `globals.yml`

On active Swift et on indique √† Kolla **comment trouver les disques**.

```yaml
# Swift - Object Storage Options
enable_swift: "yes"
# Active le d√©ploiement de Swift dans Kolla-Ansible.
# Sans √ßa, m√™me si ton disque est pr√™t, Swift ne sera pas d√©ploy√©.

# Le disque est d√©tect√© par label XFS
swift_devices_match_mode: "strict"
# "strict" = Swift ne consid√®rera comme valide que les devices correspondant EXACTEMENT
# au crit√®re donn√© (label ou nom).
# √áa √©vite qu'un mauvais disque se retrouve utilis√© par erreur.

swift_devices_name: "SWIFT_DATA"
# Comme on a "match_mode: strict", Kolla va chercher un FS XFS dont le LABEL est "SWIFT_DATA".
# Ce LABEL est celui que tu as donn√© avec mkfs.xfs -L SWIFT_DATA.
# Du coup, Kolla sait que /srv/node/sdb1 correspond √† ce device Swift.

# enable_swift_s3api: "no"
# Optionnel : permettrait d'exposer une API S3 compatible au-dessus de Swift.
# Tu pourras l'activer plus tard si tu veux un endpoint S3.
```

---

# üß± 4. Installer les outils Swift *sur l‚Äôh√¥te* (pour cr√©er les rings)

Ici, l‚Äôobjectif est d‚Äôavoir sur l‚Äôh√¥te les commandes `swift-ring-builder` pour construire les rings.

```bash
apt update
apt install -y swift swift-account swift-container swift-object
# Installe les packages Swift c√¥t√© h√¥te (hors containers) :
# - swift             : outils de base (swift, swift-ring-builder, etc.)
# - swift-account     : binaire pour g√©rer les comptes
# - swift-container   : idem pour les containers
# - swift-object      : idem pour les objets
#
# C'est JUSTE pour pouvoir cr√©er les rings (*.builder, *.ring.gz) sur l'h√¥te, 
# dans /etc/kolla/config/swift. Kolla les copiera dans les containers.
```

Si probl√®me DNS :

```bash
sudo bash -c 'echo -e "nameserver 8.8.8.8\nnameserver 1.1.1.1" > /etc/resolv.conf'
# Remplace le contenu de /etc/resolv.conf par des DNS publics (Google et Cloudflare).
# Utile si ton /etc/resolv.conf pointe vers un DNS local qui ne marche pas.
```

---

# üîÅ 5. Cr√©er les Swift rings dans `/etc/kolla/config/swift`

Les **rings** sont la ‚Äúcarte de routage‚Äù de Swift :
ils disent **quel device** (sdb1 sur telle IP, tel port) stocke **quelles partitions logiques**.

```bash
sudo mkdir -p /etc/kolla/config/swift
cd /etc/kolla/config/swift
# R√©pertoire o√π Kolla va chercher les fichiers *.builder et *.ring.gz.
# On cr√©e et g√®re les rings ici c√¥t√© h√¥te.
```

## 5.1. Account Ring

```bash
sudo swift-ring-builder account.builder create 10 1 1
# Cr√©e le fichier "account.builder" avec ces param√®tres :
# - 10 : "part_power" ‚Üí nombre de partitions = 2^10 = 1024 partitions logiques
# - 1  : "replica_count" ‚Üí 1 copie de chaque partition (lab simple, pas de HA)
# - 1  : "min_part_hours" ‚Üí d√©lai minimum en heures avant qu'une partition puisse
#        √™tre d√©plac√©e √† nouveau (rebalance trop fr√©quent prot√©g√©).
```

```bash
sudo swift-ring-builder account.builder add \
  --region 1 --zone 1 \
  --ip 9.11.93.4 --port 6001 \
  --device sdb1 --weight 100
# Ajoute un "device" dans le ring des ACCOUNT :
# - region 1 / zone 1 : structure logique du DC (tu peux en avoir plusieurs en prod)
# - ip 9.11.93.4      : IP de ton noeud de stockage (la VM AIO)
# - port 6001         : port Swift account-server
# - device sdb1       : nom logique du device (doit correspondre au device mont√©)
# - weight 100        : poids relatif pour la distribution des partitions
#
# En gros, on dit : "le service account de Swift est sur 9.11.93.4:6001 sur le disque sdb1".
```

```bash
sudo swift-ring-builder account.builder rebalance
# Calcule et √©crit dans account.ring.gz la distribution des 1024 partitions
# sur les devices configur√©s.
# C'est le "compilateur" du ring : il g√©n√®re le fichier binaire final utilis√© en prod.
```

## 5.2. Container Ring

M√™me principe, mais pour les containers :

```bash
sudo swift-ring-builder container.builder create 10 1 1
# Cr√©e un ring pour les CONTAINERS (m√™me part_power, replicas, etc.)

sudo swift-ring-builder container.builder add \
  --region 1 --zone 1 \
  --ip 9.11.93.4 --port 6002 \
  --device sdb1 --weight 100
# D√©clare que les metadata des containers seront sur :
# IP 9.11.93.4, port 6002 (container-server), disque sdb1.

sudo swift-ring-builder container.builder rebalance
# G√©n√®re container.ring.gz √† partir du builder.
```

## 5.3. Object Ring

Et enfin pour les objets eux-m√™mes :

```bash
sudo swift-ring-builder object.builder create 10 1 1
# Cr√©e le ring pour les OBJECTS (donn√©es elles-m√™mes).

sudo swift-ring-builder object.builder add \
  --region 1 --zone 1 \
  --ip 9.11.93.4 --port 6000 \
  --device sdb1 --weight 100
# D√©clare que les objets seront stock√©s sur :
# IP 9.11.93.4, port 6000 (object-server), disque sdb1.

sudo swift-ring-builder object.builder rebalance
# Calcule la r√©partition des partitions d'objets ‚Üí object.ring.gz.
```

V√©rifier l‚Äô√©tat des rings :

```bash
swift-ring-builder account.builder
swift-ring-builder container.builder
swift-ring-builder object.builder
# Affiche un r√©sum√© : nombre de partitions, r√©partition, poids, etc.
# Permet de v√©rifier qu'il n'y a pas de partitions "sans device".
```

V√©rifier les fichiers `.ring.gz` :

```bash
ls -lh /etc/kolla/config/swift/*.ring.gz
# V√©rifie que les trois fichiers binaires ont bien √©t√© g√©n√©r√©s :
# - account.ring.gz
# - container.ring.gz
# - object.ring.gz
# Ce sont ceux-l√† qui seront copi√©s DANS les containers Swift.
```

---

# üöÄ 6. (Re)d√©ployer Swift avec kolla-ansible

Toujours dans le **virtualenv** :

```bash
sudo -i
source /root/kolla-openstack/bin/activate
cd /etc/kolla
# On se place dans le r√©pertoire o√π se trouvent globals.yml + all-in-one.
```

### 6.1. Pr√©checks

```bash
kolla-ansible -i ./all-in-one prechecks --tags swift
# Lancement des pr√©-v√©rifications Kolla pour les services marqu√©s "swift".
# V√©rifie :
# - que les hosts sont accessibles
# - que la config est coh√©rente
# - que les rings existent l√† o√π ils doivent
# Si √ßa √©choue ici, √ßa t'√©vite un "deploy" qui part en vrille.
```

### 6.2. D√©ploiement Swift uniquement

```bash
kolla-ansible -i ./all-in-one deploy --tags swift
# D√©ploie / reconfigure uniquement les services li√©s √† Swift :
# - swift_proxy_server
# - swift_account_server
# - swift_container_server
# - swift_object_server
# - et leurs workers (auditor, replicator, updater, etc.)
# Utilise :
# - /etc/kolla/globals.yml pour la config globale
# - /etc/kolla/config/swift/*.ring.gz pour les rings
```

### 6.3. Post-deploy (si pas d√©j√† fait)

```bash
kolla-ansible post-deploy
# √âtape standard Kolla :
# - cr√©e /etc/kolla/admin-openrc.sh
# - configure quelques scripts de post-install
# - met en place des fichiers utilitaires
# √Ä faire au moins une fois apr√®s un d√©ploiement global.
```

### 6.4. V√©rifier les containers Swift

```bash
docker ps --format "table {{.Names}}\t{{.Status}}" | grep swift
# Liste les containers dont le nom contient "swift" avec leur statut.
# Tu dois voir :
#   swift_proxy_server, swift_account_server, swift_container_server,
#   swift_object_server, leurs workers (auditor, replicator, updater, expirer...)
# en statut "Up".
# Un container en "Restarting" = probl√®me dans les logs √† inspecter.
```

---

# üîç 7. V√©rifier que les rings sont bien dans les containers

```bash
docker exec -it swift_proxy_server   ls -l /etc/swift/*.ring.gz
docker exec -it swift_account_server ls -l /etc/swift/account.ring.gz
docker exec -it swift_container_server ls -l /etc/swift/container.ring.gz
docker exec -it swift_object_server    ls -l /etc/swift/object.ring.gz
# On v√©rifie DANS les containers :
# - que les fichiers *.ring.gz ont bien √©t√© copi√©s depuis /etc/kolla/config/swift
# - qu'ils appartiennent √† l'utilisateur swift:swift
# Sans ces rings, les daemons Swift ne savent pas sur quels devices √©crire/lire.
```

---

# üß™ 8. Tests c√¥t√© OpenStack (CLI)

```bash
source /etc/kolla/admin-openrc.sh
# Charge les variables d'environnement pour le client OpenStack :
# - OS_AUTH_URL, OS_USERNAME, OS_PASSWORD, OS_PROJECT_NAME, etc.
# Sans √ßa, la commande "openstack" ne sait pas sur quel endpoint parler ni avec quel token.
```

```bash
openstack service list | grep -i object
# V√©rifie que le service "object-store" existe dans Keystone.
# C'est l'enregistrement du service Swift dans le catalogue.
```

```bash
openstack endpoint list | grep -i object
# V√©rifie que des endpoints publics/internal/admin existent pour "object-store".
# S'il n'y a pas d'endpoint, Horizon/CLI ne pourront pas acc√©der √† Swift.
```

```bash
openstack container create demo
# Cr√©e un container Swift nomm√© "demo" dans ton projet courant.
# (√©quivalent d'un "bucket" dans S3).
```

```bash
openstack object create demo /etc/hosts
# Uploade le fichier /etc/hosts dans le container "demo".
# √áa d√©clenche :
# - un appel au Swift proxy
# - qui consulte les rings
# - qui envoie la requ√™te vers object-server/container-server/account-server
#   sur ton disque sdb1.
```

```bash
openstack object list demo
# Liste les objets pr√©sents dans le container "demo".
# Tu dois voir /etc/hosts (le nom exact d√©pend de la commande pr√©c√©dente).
```

```bash
openstack object save demo /etc/hosts --file /tmp/hosts-downloaded
# T√©l√©charge l'objet stock√© dans "demo" sous le nom /etc/hosts
# et le sauvegarde localement dans /tmp/hosts-downloaded.
```

```bash
diff /etc/hosts /tmp/hosts-downloaded
# Compare le fichier original et celui t√©l√©charg√©.
# Pas de diff√©rence => le cycle upload / stockage / download fonctionne correctement.
```

En cas d‚Äôerreur 503 ou autre :

```bash
docker logs swift_proxy_server --tail 50
docker logs swift_account_server --tail 50
docker logs swift_container_server --tail 50
docker logs swift_object_server --tail 50
# On regarde les logs des diff√©rents services Swift pour voir :
# - erreurs de permission (UID / chown)
# - erreurs de montage (/srv/node/sdb1 non mont√©)
# - erreurs de ring manquant (pas de *.ring.gz)
```

---

# üåê 9. Tests dans Horizon

1. Aller sur Horizon (`http://9.11.93.4` ou IP de ton VIP).
2. Se connecter en `admin` (ou un autre projet).
3. Menu **Project ‚Üí Object Store ‚Üí Containers**.
4. Cr√©er un container `demo`.
5. Uploader un fichier.

Derri√®re cette interface graphique, c‚Äôest **exactement** les m√™mes m√©canismes :

* Horizon ‚Üí appelle l‚ÄôAPI OpenStack Swift (via Keystone).
* Swift Proxy ‚Üí lit les rings (`*.ring.gz`).
* Swift Proxy ‚Üí envoie la requ√™te aux bons *account/container/object-server* sur ton disque.
