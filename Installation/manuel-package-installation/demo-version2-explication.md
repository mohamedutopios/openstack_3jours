## 0. Topologie VirtualBox + netplan

### ğŸ¯ Objectif

* Avoir une **VM unique** qui joue :

  * **controller** (Keystone, Glance, Nova-api, Neutron-server, Horizonâ€¦)
  * **compute** (nova-compute, libvirt, KVM/QEMU)
* Dans **VirtualBox**, donc **pas de vrai rÃ©seau physique** â†’ on simule tout.

### ğŸ”§ Ce quâ€™on met en place

* **Carte 1 â€“ NAT (`enp0s3`)**
  Sert uniquement Ã  **sortir sur Internet** : `apt`, `wget`, etc.
  â†’ IP automatique via DHCP (ex : `10.0.2.15`)

* **Carte 2 â€“ Host-only (`enp0s8`)**
  Câ€™est le **vrai rÃ©seau OpenStack** :

  * IP de management : `192.168.56.10`
  * API OpenStack accessibles depuis ta machine hÃ´te
  * RÃ©seau provider pour les VMs (les instances auront des IP dans `192.168.56.0/24`)

Dans Netplan :

* `enp0s3` en DHCP â†’ route par dÃ©faut vers Internet
* `enp0s8` IP fixe `192.168.56.10/24` â†’ pas de route par dÃ©faut, juste du L2 / L3 local

### ğŸ¤” Pourquoi ?

* **NAT** : tu nâ€™as rien Ã  configurer, Ã§a marche partout.
* **Host-only** : ta machine hÃ´te et ta VM OpenStack sont dans le **mÃªme LAN virtuel** :

  * tu accÃ¨des Ã  Horizon via `http://192.168.56.10/horizon`
  * tu ping et SSH sur les VMs Cirros depuis ton laptop.

---

## 1. PrÃ©paration OS + dÃ©pÃ´ts OpenStack

### ğŸ”§ Ce quâ€™on fait

1. `apt update && upgrade` â†’ OS Ã  jour.
2. `hostnamectl set-hostname controller` + `/etc/hosts` :

   * rÃ©soudre `controller` â†’ `192.168.56.10`
3. NTP (`chrony`) â†’ Ã©viter les problÃ¨mes de tokens / horodatage.
4. Ajout du **cloud-archive:caracal** â†’ paquets OpenStack 2024.1.
5. Installation du `python3-openstackclient`.

### ğŸ¤” Pourquoi ?

* OpenStack est **trÃ¨s sensible Ã  la rÃ©solution DNS/hostname**.
  Tous les fichiers sont bourrÃ©s de `controller` â†’ il faut que Ã§a rÃ©solve vers lâ€™IP mgmt.
* Si lâ€™heure dÃ©rive â†’ keystone refuse des tokens (problÃ¨mes de TTL).
* cloud-archive = paquets OpenStack **supportÃ©s par Canonical**, pas une PPA obscure.
* `openstackclient` = ton **couteau suisse** : tu testes chaque brique avec Ã§a.

---

## 2. Services de base : MariaDB, RabbitMQ, Memcached, etcd

> Tous les services OpenStack ont besoin de **stockage**, de **messagerie** et souvent de **cache**.

### ğŸ”§ MariaDB

* Fichier `99-openstack.cnf` :

  * `bind-address = 192.168.56.10` â†’ Ã©coute sur lâ€™IP management
  * `innodb_file_per_table`, `utf8` â†’ tuning classique OpenStack
  * `max_connections` augmentÃ© â†’ beaucoup de services se connectent.

ğŸ‘‰ **RÃ´le** : toutes les bases (`keystone`, `glance`, `nova`, `neutron`, `cinder`, â€¦) y vivent.

### ğŸ”§ RabbitMQ

* CrÃ©ation de lâ€™utilisateur `openstack` avec un mot de passe (`RABBIT_PASS`).
* Tous les services (Nova, Neutron, Cinderâ€¦) sâ€™y connectent via `transport_url = rabbit://...`.

ğŸ‘‰ **RÃ´le** : bus de messages, coordination asynchrone (RPC, Ã©vÃ©nements).

### ğŸ”§ Memcached

* Ã‰coute sur `192.168.56.10`.
* UtilisÃ© par Keystone et Horizon pour **mettre en cache les tokens et sessions**.

ğŸ‘‰ **RÃ´le** : performance â†’ Ã©viter de taper la DB en permanence.

### ğŸ”§ etcd

* Store clÃ©/valeur distribuÃ©.
* UtilisÃ© par Nova/Neutron entre autres pour le **locking, coordination, metadata** moderne.

ğŸ‘‰ **RÃ´le** : remplacer certains usages de DB pour la coordination.

---

## 3. Keystone (Identity) + Apache/WSGI

### ğŸ¯ Objectif

Avoir un **service dâ€™identitÃ© central** qui gÃ¨re :

* utilisateurs
* projets
* rÃ´les
* tokens dâ€™authentification

### ğŸ”§ Ce quâ€™on met en place

1. **DB `keystone`** dans MariaDB.
2. Paquets `keystone`, `apache2`, `libapache2-mod-wsgi-py3`.
3. `keystone.conf` :

   * `[database]` â†’ URL MySQL
   * `[token] provider = fernet` â†’ tokens chiffrÃ©s symÃ©triquement
4. `keystone-manage db_sync`, `fernet_setup`, `credential_setup` :

   * crÃ©ation des tables
   * gÃ©nÃ©ration des secrets pour chiffrer les tokens
5. `keystone-manage bootstrap` :

   * crÃ©ation de lâ€™**admin**, du **tenant admin**, de la rÃ©gion, des endpoints par dÃ©faut.
6. VirtualHost Apache sur **port 5000** :

   * `WSGIScriptAlias / /usr/bin/keystone-wsgi-public`
   * Apache joue le rÃ´le de **front HTTP** pour Keystone.

### ğŸ¤” Pourquoi Apache + WSGI ?

Historiquement :

* Keystone est une app Python WSGI.
* Apache gÃ¨re :

  * les workers,
  * la montÃ©e en charge,
  * les logs HTTP,
  * la TLS potentielle.

Aujourdâ€™hui certains services OpenStack utilisent `uwsgi` ou des api-servers propres, mais Keystone en APT sur Ubuntu reste sous Apache.

### ğŸ’¡ RÃ©sultat

Tu peux faire :

```bash
source admin-openrc.sh
openstack token issue
openstack user list
```

â†’ tu as une **PKI dâ€™auth centralisÃ©e** pour tout le cloud.

---

## 4. Glance (Image Service)

### ğŸ¯ Objectif

Avoir un dÃ©pÃ´t dâ€™images (QCOW2, RAWâ€¦) que Nova utilisera pour crÃ©er les disques des instances.

### ğŸ”§ Ce quâ€™on met en place

1. DB `glance`.
2. Utilisateur `glance` + projet `service` + rÃ´le `admin` pour ce service.
3. Service `image` + endpoints (public/internal/admin sur port **9292**).
4. `glance-api.conf` :

   * `[database]` â†’ URL SQL
   * `[keystone_authtoken]` â†’ comment Glance parle Ã  Keystone
   * `[glance_store]` â†’ `stores=file`, rÃ©pertoire `/var/lib/glance/images/`
5. `glance-manage db_sync` â†’ crÃ©ation des tables.

Puis on **uploade une image Cirros** pour tester.

### ğŸ¤” Pourquoi file backend ?

* Simple.
* Suffisant pour un lab AIO.
* En prod tu utiliserais Swift, Ceph RBD ou autre.

---

## 5. Nova + Placement (Compute)

### ğŸ¯ Objectif

* **Nova** = gestion des instances (VM), planification, lifecycle.
* **Placement** = inventaire des ressources (vCPU, RAM, disque) et scheduling plus fin.

### ğŸ”§ Ce quâ€™on met en place

1. DBs :

   * `nova_api`, `nova`, `nova_cell0`, `placement`
   * câ€™est important pour la notion de **cells v2** (scalabilitÃ© horizontale).

2. Utilisateurs `nova` et `placement` + services + endpoints.

3. `placement-api` + Apache (port 8778) avec DB propre :

   * gÃ¨re la **description des ressources** et des allocations.

4. Installation des services Nova cÃ´tÃ© controller :

   * `nova-api`, `nova-scheduler`, `nova-novncproxy`, `nova-conductor`

5. Fichier **central** `/etc/nova/nova.conf` :

   * `[DEFAULT] my_ip = 192.168.56.10` â†’ IP mgmt
   * `transport_url` â†’ RabbitMQ
   * `use_neutron = True` â†’ rÃ©seau gÃ©rÃ© par Neutron
   * `[api_database]` / `[database]` â†’ DBs Nova
   * `[keystone_authtoken]` â†’ authentification vers Keystone
   * `[vnc]` :

     * `server_listen = 0.0.0.0`
     * `novncproxy_base_url = http://controller:6080/vnc_auto.html`
       â†’ pour accÃ©der Ã  la console graphique des VMs depuis Horizon
   * `[glance]` â†’ URL du service image
   * `[placement]` â†’ comment Nova parle Ã  Placement

6. **Cells v2** :

```bash
nova-manage api_db sync
nova-manage cell_v2 map_cell0
nova-manage cell_v2 create_cell --name=cell1
nova-manage db_sync
```

â¡ Ã§a prÃ©pare Nova Ã  gÃ©rer potentiellement **plusieurs groupes de compute nodes** (cells).

7. Installation cÃ´tÃ© **compute** (mÃªme nÅ“ud dans AIO) :

   * `nova-compute`
   * `nova-compute.conf` â†’ `[libvirt] virt_type = qemu`

### ğŸ¤” Pourquoi `virt_type = qemu` ?

* Tu es dans une VM VirtualBox â†’ **pas de virtualisation imbriquÃ©e (nested)** fiable.
* KVM ne marche pas bien (ou pas du tout).
* `qemu` = full virtualisation software : plus lent, mais Ã§a fonctionne partout.

### ğŸ” RÃ©sultat

Avec `openstack compute service list` + `openstack hypervisor list` :

* tu vois ton hyperviseur (`controller`) dÃ©clarÃ©
* `nova-compute` UP
  â†’ tu peux lancer des VMs.

---

## 6. Neutron (rÃ©seau) + sysctl

### ğŸ¯ Objectif

Fournir :

* des rÃ©seaux L2/L3
* des IPs pour les instances
* du DHCP, du NAT Ã©ventuel
* de la sÃ©curitÃ© (Security Groups)

Dans ta recette : **un rÃ©seau provider flat** sur `enp0s8`.

### ğŸ”§ Ce quâ€™on met en place

1. DB `neutron`, utilisateur `neutron`, service + endpoints.

2. Services installÃ©s :

   * `neutron-server` (API Neutron)
   * `neutron-plugin-ml2` (framework de plugins)
   * `neutron-linuxbridge-agent` (L2)
   * `neutron-dhcp-agent`
   * `neutron-metadata-agent`
   * `neutron-l3-agent` (routing/NAT)

3. `neutron.conf` :

   * `[DEFAULT]` :

     * `core_plugin = ml2` â†’ abstraction rÃ©seau
     * `service_plugins = router` â†’ support L3
     * `transport_url` â†’ RabbitMQ
     * `notify_nova_on_*` â†’ synchro ports / instances
   * `[database]` â†’ DB Neutron
   * `[keystone_authtoken]` â†’ auth
   * `[nova]` â†’ interaction pour la gestion de ports attachÃ©s aux VMs

4. ML2 (`ml2_conf.ini`) :

   * `type_drivers = flat,vlan`
   * `tenant_network_types =` (vide â†’ on ne gÃ¨re que provider)
   * `mechanism_drivers = linuxbridge`
   * `flat_networks = provider`
     â†’ tu dis : *il existe une â€œphysnetâ€ appelÃ©e provider, en type flat.*

5. Linuxbridge (`linuxbridge_agent.ini`) :

   * `physical_interface_mappings = provider:enp0s8`
     â†’ physiquement, la physnet `provider` correspond Ã  ta NIC `enp0s8`.

6. L3 / DHCP / metadata : configuration standard.

7. CÃ´tÃ© Nova (`nova.conf`, section `[neutron]`) :

   * Nova sait oÃ¹ est lâ€™API Neutron, avec quels credentials.
   * `service_metadata_proxy` + `metadata_proxy_shared_secret` :
     â†’ permet aux VMs de parler au service `metadata` via Neutron.

### ğŸ§  sysctl pour Neutron

Tu ajoutes :

```bash
net.ipv4.ip_forward=1
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
```

ğŸ‘‰ **Pourquoi ?**

* `ip_forward=1` â†’ autoriser le routage IP (sinon ton L3 agent ne route pas).
* `rp_filter=0` â†’ Ã©viter que le kernel considÃ¨re certains paquets comme spoofÃ©s (important avec les bridges / virtualisation).
* `bridge-nf-call-iptables=1` â†’ pour que les paquets qui passent dans les bridges Linux soient filtrÃ©s par iptables (Security Groups).

---

## 7. Horizon (Dashboard)

### ğŸ¯ Objectif

Avoir une interface web graphique pour administrer ton cloud.

### ğŸ”§ Ce quâ€™on met en place

* `openstack-dashboard` (Horizon)
* `local_settings.py` :

  * `OPENSTACK_HOST = "controller"` â†’ parle au Keystone sur `controller:5000`
  * `ALLOWED_HOSTS = ['*']` â†’ pour ne pas se faire bloquer
  * `TIME_ZONE = 'Europe/Paris'`

Horizon est un **projet Django** :

* sert les pages via Apache
* utilise Keystone pour authentification
* parle ensuite aux autres services (Nova, Neutron, Glanceâ€¦).

---

## 8. Cinder (optionnel, stockage bloc LVM)

### ğŸ¯ Objectif

Permettre de crÃ©er **des volumes bloc** (comme des disques supplÃ©mentaires) que :

* tu peux attacher aux instances
* tu peux snapshotter, dÃ©tacher, rÃ©attacherâ€¦

### ğŸ”§ Ce quâ€™on met en place

1. On simule un disque avec un `loop` :

   * fichier `/var/lib/cinder.img`
   * attachÃ© Ã  `/dev/loop2`
   * `pvcreate` + `vgcreate cinder-volumes`

2. DB `cinder`, utilisateur `cinder`, service, endpoints.

3. Installation `cinder-api`, `cinder-scheduler`, `cinder-volume`.

4. `cinder.conf` :

   * `[database]` â†’ DB Cinder
   * `[DEFAULT]` :

     * `transport_url` â†’ RabbitMQ
     * `enabled_backends = lvm`
     * `glance_api_servers = http://controller:9292`
   * `[lvm]` :

     * `volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver`
     * `volume_group = cinder-volumes`
   * `[oslo_concurrency] lock_path` â†’ lock des opÃ©rations.

5. IntÃ©gration Nova :

   * section `[cinder] os_region_name = RegionOne` dans `nova.conf`.

### ğŸ¤” RÃ©sultat

Tu peux :

```bash
openstack volume create --size 1 demo-volume
openstack volume list
```

Puis attacher ce volume Ã  une VM.

---

## 9. Test de bout en bout : lancer une VM

Tout ce que tu as fait avant sert Ã  Ã§a ğŸ‘‡

1. **Flavor** (Nova) â†’ dÃ©crit les ressources virtuelles.
2. **Image** (Glance) â†’ disque de base.
3. **Network** (Neutron) â†’ provider flat `192.168.56.0/24`.
4. **Keypair** (Nova + SSH) â†’ accÃ¨s Ã  la VM.
5. **Security Groups** (Neutron) â†’ autoriser ICMP + SSH.
6. `openstack server create` â†’ Nova orchestre :

   * parle Ã  Glance â†’ tÃ©lÃ©charge lâ€™image
   * rÃ©serve des ressources via Placement
   * demande Ã  Neutron un port rÃ©seau
   * crÃ©e et boote la VM via libvirt/QEMU
   * publie les infos dans Keystone pour Horizon, etc.

Ensuite tu **ping** et tu **SSH** depuis ta machine hÃ´te vers lâ€™IP de la VM (dans le pool 192.168.56.100â€“200).


